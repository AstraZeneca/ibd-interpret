# Dual Stream Multiple Instance Learning (DSMIL) Instructions

Adapted from https://github.com/binli123/dsmil-wsi

# 0) Directory structure

Directory should be organised like this:

Slides organised as:
```
root
|-- WSI
|   |-- SLIDE_1.svs
|   |-- SLIDE_2.svs
|   |-- ...
|-- WSI_patches
|   |-- DATASET_1
|   |   |-- SLIDE_1
|   |   |   |-- PATCH_1.jpeg
|   |   |   |-- PATCH_2.jpeg
|   |   |   |-- ...
|-- datasets
|   |-- DATASET_1
|   |   |-- SLIDE_1.csv
|   |   |-- SLIDE_2.csv
|-- splits
|   |-- 0
|   |   |-- X_train.csv
|   |   |-- X_test.csv
|   |   |-- y_train.csv
|   |   |-- y_test.csv
|   |-- 1
|   |   |-- X_train.csv
|   |   |-- ...
|-- MIL_RESULTS
|   |-- MODEL_1
|   |   |-- RESULTS.txt
|   |   |   |-- ...
|-- attention_maps
|   |-- MODEL_1
|   |   |-- SLIDE_1.png
```

* WSI is where the WSIs are stored. 
* WSI_patches holds directories of patches that are considered different datasets. For example, dataset 1 has patches extracted at 20x, dataset 2 at 40x etc
* datasets contains the embeddings that are computed using the trained embedder. Each slide has a csv file where the rows are the patches (patch 1, 2...) and the columns are each a dimension of the embeddings for that patch. For example, if there are 200 patches and an embedding dim of 512, then the csv will be a 200x512 matrix
* MIL_RESULTS is where the results of weakly supervised classification will be stored, there will be a .txt file with the per-fold performance and a performance summary
* attention_maps is where attention maps generated by trained models will be stored

# 1) Setup environment

```
conda env create --name dsmil --file env.yml
conda activate dsmil
conda install -c conda-forge openslide
```

# 2) Patch Extraction

In theory you can use any patch extraction method, to use the author's:

```
python deepzoom_tiler.py \
-p "path_to_wsi" \
-d "path_to_patches" \
-j 8 -m 0 -b 20 -v "tiff" -o 40 -s 224
```

There are many parameters for controlling patch extraction - see the arguments in deepzoom_tiler.py

# 3) Train SimCLR

```
python run.py --dataset='path_to_patches'
```

There is a config file for controlling training parameters, the trained models will be stored in simclr/runs

# 4) Compute embeddings

Once simclr is trained, use to model to compute patch embeddings for all WSIs

```
python compute_feats.py \
--dataset="path_to_patches" \
--magnification="single" \
--weights="weights_in_simclr/runs"
```

# 5) Define CV splits and train MIL aggregator

The MIL aggregator is set up to do k-fold cross validation. To do this, define cross validation splits in a splits/ directory. The files X_train, and X_test simply contain a list of the slide IDs and y_train, y_test contain the labels (0 or 1 for example). These files can be easily generated with sklearn.

Once splits are generated:

```
python MIL_CV.py \
--num_epochs=200 \
--feats_size=512 \
--dataset="embeddings_dataset" \
--cv_splits="path_to_CV_splits" \
--results_dir="MIL_results/normal_lesional/5x" \
--save_name="results_file_name" \
--num_classes=1
```

The results of training will be stored in MIL_RESULTS as a txt file


# 6) Generate attention maps

Once both the embedder and aggregator are trained, attention maps can be generated. Attention maps will be stored in attention_maps/MODEL_1/fused

There are many options for controlling the attention map generation - see the arguments in attention_map.py

```
python attention_map.py --embedder_weights="simclr/runs/model.pth" \
--aggregator_weights="weights/model.pth" \
--bag_path="path_to_patches" \
--gt_path="path_to_ground_truth" \
--map_path="path_to_attention_maps" \
--wsi_path="path_to_wsi" \
--feats_path="path_to_embeddings" \
--format=".tiff" \
--scale=16 \
--threshold=0.0 \
--patch_origin="histolab" \
--level=0 \
--export_scores=1 \
--num_classes=1 \
--thres 0.5 \
--class_name "positive"
```